{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Ensembling Methods\n",
    "https://www.kaggle.com/amrmahmoud123/1-guide-to-ensembling-methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Error \n",
    "* Bias: how much on an average are the **predicted values different from the actual value**  <br>\n",
    "= 목표값과 실제값이 얼마나 많이 떨어져 있는지\n",
    "\n",
    "\n",
    "* Variance: how are the prediction made on **same observation different from each other**   <br>\n",
    "= 관측값들끼리 얼마나 멀리 떨어져 있는지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"undervsover.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ensemble? \n",
    "* Ensemble learning is one way to excute this **trade off analysis**  <br> \n",
    "= 모델이 이 두 가지 에러를 트레이드오프 할 수 있는 최적의 방법을 찾는 것   \n",
    "\n",
    "\n",
    "* **The wisdowm of the crowd** 집단지성!   <br> \n",
    "= 여러 개의 분류기(Classifier)를 생성하고, 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Max Voting \n",
    "* 보통 classification 문제에 사용\n",
    "* 서로 다른 알고리즘에서의 예측값을 보고, 가장 많이 투표된 쪽으로 결정하는 방식 \n",
    "\n",
    "\n",
    "1. **Hard Voting** (=Majority Voting): 가장 최빈값으로 투표 ex) y=mode{0,0,1}=0\n",
    "2. **Soft Voting**: it gives more weight to highly confident votes, 가능성이 더 높은 것에 가중치를 더 주는 방식  <br>\n",
    "분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서, 이들 중 확률이 가장 높은 레이블 값을 최종 voting 결과값으로 선정   <br>\n",
    "일반적으로 soft voting이 voting 방법으로 적용됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Averaging \n",
    "* 보통 regression, 혹은 classification에서 probability 계산할 때 사용 \n",
    "* 예측값들의 평균을 내는 방식 \n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "\n",
    "+) Weighted Average\n",
    "* 모델에 가중치를 다르게 줘서, 가중평균으로 계산하는 방식 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Advanced Ensemble Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bagging \n",
    "* bootstrap aggregating의 줄임말 \n",
    "* bootstrap: 여러 개의 작은 데이터셋을 임의로 만들어 개별 평균의 분포도를 측정하는 등의 목적을 위한 샘플링 방식 \n",
    "\n",
    "\n",
    "* 각각의 분류기가 모두 같은 유형의 알고리즘이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것   \n",
    "    1) dataset을 복원추출해 여러 subset을 만든다   \n",
    "    2) 각각 subset에서 여러 모델(classifier)를 적합시킨다   \n",
    "    3) 각각의 모델은 parallel하고 independent   \n",
    "    4) 각각의 모델에서 예측값을 생성한다   \n",
    "    5) 이를 모아서, (voting이나 averaging 방식을 통해) 최종 예측을 진행한다   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1, **Bagging meta-estimator**:   \n",
    "classification(BaggingClassifier), regression(BaggingRegressor)\n",
    "\n",
    "\n",
    "steps   \n",
    "1) original dataset에서 random subset이 생성된다 (bootstrapping)   \n",
    "2) subset들은 모든 feature를 포함한다   \n",
    "3) user-specified base estimator로 각각의 smaller set들을 예측한다   \n",
    "4) 이러한 예측 결과를 합쳐 최종 결과로 만든다   \n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "parameter\n",
    "* base_estimator: random subset에 적용될 base estimator를 정한다 - default: decision tree\n",
    "* n_estimator: base estimator의 개수 \n",
    "* max_samples: subset의 size를 결정\n",
    "* max_features: subset의 max feature 개수를 결정\n",
    "* n_jobs: 동시에 수행되는 system core의 개수, -1일때는 컴퓨팅 파워 최대로 돌린다 \n",
    "* random_state: random split 방식 지정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2, **Random Forest**:    \n",
    "random forest *randomly* selects a set of features which are used to decide the best split at each node of the decision tree    \n",
    "여러 개의 결정 트리 분류기가 전체 데이터에서 각자의 데이터를 샘플링해서, 개별적으로 학습을 수행한 뒤 최종적으로 보팅을 통해 예측 결정 \n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "steps   \n",
    "1) original dataset에서 random subset이 생성된다 (bootstrapping)     \n",
    "2) 각각의 decision tree에서, feature를 랜덤으로 택해 best split을 결정한다     \n",
    "3) 모든 decision tree의 평균을 계산해 마지막 최종 결정을 한다     \n",
    "\n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "parameter\n",
    "* n_estimator: random forest에서 만들어 낼 decision tree의 개수 결정 \n",
    "* criterion: split 결정계수 (ex. gini, entropy) \n",
    "* max_features: 각각의 decision tree에서 최대로 사용될 feature의 개수,   \n",
    "max_feature가 커지면 성능이 좋아지지만 트리의 다양성이 저하될 수 있다 \n",
    "* max_depth: 최대 깊이 결정 \n",
    "* min_samples_split: 샘플의 최소치 \n",
    "* min_samples_leaf: 가장 하단 leaf의 개수,    \n",
    "작은 leaf size는 training data에서 더욱 쉽게 noise를 잡을 수 있다 (but overfitting 가능성이 생긴다) \n",
    "* max_leaf_nodes: 최대로 뻗어 나갈 node 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Boosting \n",
    "* **converts weak learner to strong learners**   \n",
    "= 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식 \n",
    "* to train weak learners sequentially, each trying to correct its predecessor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost \n",
    "* Adaptive Boosting \n",
    "* 오류 데이터에 가중치를 부여하면서, 제대로 예측하게끔 부스팅 수행 \n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "steps    \n",
    "1) 모든 데이터의 관측값에는 동일한 가중치가 부여된다    \n",
    "2) subset에 대해 모델이 생성된다   \n",
    "3) 이 모델을 사용하여 전체 데이터에 대한 예측을 진행한다   \n",
    "4) 예측값과 실제값을 비교해 에러를 계산한다   \n",
    "5) **잘못 예측한 데이터에 더 높은 가중치를 부여하여, 다음 모델을 만든다**    \n",
    "6) error value를 통해 weights를 정한다 = 에러가 클수록 weight가 커진다   \n",
    "7) 에러 함수가 바뀌지 않을 때 까지, 혹은 maximum limit에 도달할 때 까지 이 과정이 반복된다 \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "parameter\n",
    "* base_estimator: 베이스 머신러닝 알고리즘 \n",
    "* n_estimator: base estimator의 수 지정, n_estimator를 높이면 더 좋은 성능을 얻을 수 있다 \n",
    "* learning_rate: 러닝레이트. n_estimator와 trade-off 관계 \n",
    "* max_depth: 각각 estimator들의 maximum depth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. stacking\n",
    "* 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다 (~ Bagging, Boosting)\n",
    "* 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행한다 (가장 큰 차이점) \n",
    "\n",
    "\n",
    "-> **개별 알고리즘의 예측 결과 data set을 최종적인 meta data set으로 만들어,      \n",
    "별도의 ML 알고리즘으로 최종 학습을 수행하고, 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
