{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from functools import partial,reduce\n",
    "from assignment2 import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data 설명\n",
    "\n",
    "1. Label: 유료 계정 등록 여부(target)\n",
    "2. bias: 회귀 모형에서의 상수항을 위한 term (추정 시 포함하지 않아도 ok)\n",
    "3. experience: 근속연수\n",
    "4. salary: 연봉\n",
    "\n",
    "\n",
    "어떤 사용자가 유료 계정을 등록할지(Label == 1)에 대한 예측을 로지스틱 회귀 모형으로 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('assignment_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"\n",
    "    한 지점에서 step size만큼 이동하는 step 함수를 구현하세요.\n",
    "    v와 direction은 벡터.\n",
    "    \"\"\"\n",
    "    \n",
    "    v2 = []\n",
    "    for v_i, d_i in zip(v,direction):\n",
    "        v2.append(v_i + step_size * d_i)\n",
    "    \n",
    "    return v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* v(i+1) = v(i) + (learning rate) * direction \n",
    "* v가 벡터이므로 각각의 인자를 받아 list 값으로 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f) :\n",
    "    \"\"\"\n",
    "    f에 대한 예외처리를 위한 함수(f가 infinite일 때)\n",
    "    \"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_bgd(target_fn, gradient_fn, theta_0, tolerance = 0.00001): # bgd: batch gradient descent\n",
    "    \"\"\"\n",
    "    목적함수를 최소화시키는 theta를 경사 하강법을 사용해서 찾는다.\n",
    "    \"\"\"\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    # 시작점 설정\n",
    "    theta = theta_0\n",
    "    target_fn = safe(target_fn) # 오류를 처리할 수 있는 target_fn으로 변환\n",
    "    value = target_fn(theta) # 최소화시키려는 값\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta) # gradient값 계산\n",
    "        next_thetas = [step(theta, gradient, i) for i in step_sizes] #### update thetas --> 각 step sizes에 따른 theta값을 list형태로 리턴\n",
    "        \n",
    "        # 함수를 최소화시키는 theta 선택\n",
    "        obj = next_thetas # 어떤 값\n",
    "        key = target_fn # 목적함수 \n",
    "        next_theta = min(obj, key = key) # key가 함수, obj 함수 안에 들어가는 값 \n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # tolerance만큼 수렴하면 멈춤\n",
    "        temp = abs(value - next_value) # temp 채워넣기\n",
    "        if temp < tolerance:\n",
    "            return theta\n",
    "        else: #### 어떻게 업데이트 시킬지 채워넣으세요\n",
    "            theta = next_theta\n",
    "            value = next_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* target_fn: 목적함수, gradient_fn: 그래디언트 함수, theta_0: theta 초기값 \n",
    "* gradient = gradient_fn(theta): theta일 때의 gradient position \n",
    "* next_thetas : 다음 지점으로 이동하기 위해 구현한 함수\n",
    "* [step(theta, graidient, i) for i in step_sizes] = theta + i(step_sizes에 있는 learning rate 값) * gradient (direction) \n",
    "\n",
    "<br>\n",
    "\n",
    "* min(obj, key=key): key 함수에서, obj 값이 key 함수에 들어가는 값이다. 이 때의 min 값이 next_theta \n",
    "* obj에서 next_thetas 안의 값들을 고려하고, 이를 통해 target_fn에 next_theta 값을 넣어 next_value 값을 찾는다. \n",
    "\n",
    "<br> \n",
    "\n",
    "* tolerance: value와 next_value 차이값이 매우 매우 작아진다면, minima 값 근처에서 거의 수렴하고 있는 형태를 띄고 있다는 것을 의미한다 \n",
    "* 따라서 temp가 tolerance보다 작다면, 더 이상 다음 값을 찾을 필요가 없으므로, 즉 수렴했다고 생각하면 되므로 theta 값을 반환한다 \n",
    "* temp가 tolerance 보다 여전히 큰 값이라면, minima 값을 찾아 갈 만한 여지가 남았다는 의미이므로 next_theta, next_value 값으로 그 다음 값을 찾는다 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic():\n",
    "    \"\"\"\n",
    "    sgd 구현 (추가적인 부분이니 필수는 아닙니다.)\n",
    "    random sampling 하는 부분(함수로 따로 구현하셔도 ok) + gd 부분\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 로지스틱 함수\n",
    "해당 함수는 1/(1+exp[-(ax+b)])로 표현되었음을 기억합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 로지스틱 함수: 해당 값에서의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    \"\"\"\n",
    "    softmax 구현\n",
    "    \"\"\"\n",
    "    sum_v = sum([math.exp(v_i) for v_i in v])\n",
    "    return [math.exp(v_i)/sum_v for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* softmax: 값의 출력, 로지스틱 함수를 통해 식을 구현하여 출력값들의 총합이 1이 되게끔 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Likelihood 구현\n",
    "그냥 Likelihood function 대신, log likelihood function을 이용해서 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood_i(x_i, y_i, beta): # 개별 데이터포인트에 대한 likelihood\n",
    "    \"\"\"\n",
    "    해당 함수에 대한 설명을 작성하고,\n",
    "    리턴문을 채우세요.\n",
    "    \"\"\"\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta))+1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood(x, y, beta): # 전체 데이터에 대한 likelihood\n",
    "    \"\"\"\n",
    "    함수의 인자를 채워넣고,\n",
    "    zip 함수를 이용하여 return 문을 완성하세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    loglikelihood = 0.0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        loglikelihood += logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "    \n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **logistic_log_likelihood_i**   \n",
    "개별 데이터포인트에 대한 likelihood  \n",
    "yi* log(f) + (1-yi)* log(1-f) (yi = 0 or 1)  \n",
    "\n",
    "\n",
    "* **logistic_log_likelihood**  \n",
    "전체 데이터에 대한 likelihood = 개별 데이터포인트에 대한 likelihood 값의 합   \n",
    "likelihood의 경우 개별 값의 곱으로 구현되는데, 여기서는 log를 취해 주었으므로 합을 구하면 됨 \n",
    "* (추가) 여기서의 logistic_log_likelihood는 위로 볼록한 형태의 함수  \n",
    "   -> 미분값=0 이 되는 값에서 최댓값이 도출됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient for Log Reg\n",
    "아래 3가지 함수에 대해 해당 함수의 인자와 기능을 자세히 설명하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_partial_ij(x_i, y_i, beta, j): # 편미분 \n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j) for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add, [logistic_log_gradient_i(x_i, y_i, beta) for x_i, y_i in zip(x,y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* i: 샘플의 개수, j: 피쳐의 개수\n",
    "* J(theta) = logistic regression의 cost function \n",
    "\n",
    "\n",
    "* cost function의 최솟값을 찾기 위해서 이러한 과정을 수행한다.   \n",
    "-> 왜? gradient descent 알고리즘의 **gradient**를 구하기 위해서 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gradient**: 함수를 구성하는 각 변수에 대한 일차 편미분값으로 구성되는 벡터."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **logistic_log_partial_ij**  \n",
    "    하나의 변수에 대한 i(행), j(열)의 gradient를 구하기 위해서 편미분 한 것 \n",
    "\n",
    "\n",
    "2. **logistic_log_gradient_i**  \n",
    "    모든 변수에 대해서 gradient를 구하기 위해 합쳐준 것  \n",
    "    각각의 피쳐에 대해 (j에 대해) gradient를 구하고 -> 이것을 열로 합쳐준 것을 본 것 \n",
    "\n",
    "\n",
    "3. **logistic_log_gradient**  \n",
    "    행을 다 합쳐준 것을 본 것 (전체 데이터셋 본 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>84000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.1</td>\n",
       "      <td>69000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>79000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  bias  experience  salary\n",
       "0        1     1         0.7   48000\n",
       "1        0     1         1.9   48000\n",
       "2        1     1         2.5   60000\n",
       "3        0     1         4.2   63000\n",
       "4        0     1         6.0   76000\n",
       "..     ...   ...         ...     ...\n",
       "195      0     1         6.5   84000\n",
       "196      0     1         6.9   73000\n",
       "197      0     1         5.1   72000\n",
       "198      1     1         9.1   69000\n",
       "199      1     1         9.8   79000\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Label', axis = 1)\n",
    "y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\venv\\jbeen\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\LG\\venv\\jbeen\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\LG\\venv\\jbeen\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\LG\\venv\\jbeen\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.05392432, -1.08449023],\n",
       "       [ 0.        ,  0.44580782,  0.63432448],\n",
       "       [ 0.        , -0.9468006 , -0.28646912],\n",
       "       [ 0.        ,  0.98142645,  0.75709695],\n",
       "       [ 0.        ,  0.08872874, -0.28646912],\n",
       "       [ 0.        , -0.33976616, -0.47062784],\n",
       "       [ 0.        , -1.16104805, -1.51419391],\n",
       "       [ 0.        ,  0.51722364,  0.81848319],\n",
       "       [ 0.        ,  1.05284226,  0.38877952],\n",
       "       [ 0.        , -1.4110034 , -1.39142143],\n",
       "       [ 0.        ,  0.01731293,  0.511552  ],\n",
       "       [ 0.        ,  0.48151573,  0.38877952],\n",
       "       [ 0.        , -1.55383504, -2.18944254],\n",
       "       [ 0.        , -1.19675595, -0.1023104 ],\n",
       "       [ 0.        ,  1.73129252, -0.96171775],\n",
       "       [ 0.        , -1.4110034 , -0.71617279],\n",
       "       [ 0.        , -1.19675595, -0.34785536],\n",
       "       [ 0.        , -0.83967687, -0.65478655],\n",
       "       [ 0.        ,  0.58863946,  0.75709695],\n",
       "       [ 0.        ,  0.16014456, -0.22508288],\n",
       "       [ 0.        , -0.01839498,  0.45016576],\n",
       "       [ 0.        ,  1.26708971,  1.18680063],\n",
       "       [ 0.        , -1.33958759, -0.71617279],\n",
       "       [ 0.        ,  0.51722364,  1.24818687],\n",
       "       [ 0.        , -0.30405825,  0.32739328],\n",
       "       [ 0.        ,  1.83841624,  0.81848319],\n",
       "       [ 0.        ,  1.62416879,  1.73927679],\n",
       "       [ 0.        ,  0.87430272,  0.38877952],\n",
       "       [ 0.        , -1.01821641, -1.45280767],\n",
       "       [ 0.        , -0.87538478, -0.71617279],\n",
       "       [ 0.        ,  0.48151573,  0.26600704],\n",
       "       [ 0.        ,  0.94571854,  0.87986943],\n",
       "       [ 0.        , -0.87538478, -0.16369664],\n",
       "       [ 0.        ,  1.1956739 ,  1.30957311],\n",
       "       [ 0.        ,  0.3386841 ,  0.57293824],\n",
       "       [ 0.        ,  0.26726828,  0.63432448],\n",
       "       [ 0.        , -1.01821641, -0.71617279],\n",
       "       [ 0.        ,  0.41009992, -0.83894527],\n",
       "       [ 0.        , -0.12551871, -0.1023104 ],\n",
       "       [ 0.        ,  0.69576318,  0.2046208 ],\n",
       "       [ 0.        , -0.62542942, -0.16369664],\n",
       "       [ 0.        , -1.51812713, -1.26864895],\n",
       "       [ 0.        , -1.51812713, -1.20726271],\n",
       "       [ 0.        ,  1.15996599,  0.02046208],\n",
       "       [ 0.        , -0.83967687, -1.08449023],\n",
       "       [ 0.        , -0.9825085 , -1.14587647],\n",
       "       [ 0.        ,  0.41009992,  0.08184832],\n",
       "       [ 0.        , -1.44671131, -1.08449023],\n",
       "       [ 0.        ,  1.44562925,  1.73927679],\n",
       "       [ 0.        ,  1.15996599,  2.2303667 ],\n",
       "       [ 0.        , -0.73255315, -1.33003519],\n",
       "       [ 0.        ,  0.58863946,  0.69571071],\n",
       "       [ 0.        ,  1.12425808,  0.81848319],\n",
       "       [ 0.        ,  1.33850553,  1.12541439],\n",
       "       [ 0.        ,  1.1956739 ,  1.18680063],\n",
       "       [ 0.        ,  1.12425808,  1.12541439],\n",
       "       [ 0.        , -0.23264243,  0.26600704],\n",
       "       [ 0.        , -0.26835034, -1.51419391],\n",
       "       [ 0.        ,  0.83859481,  0.94125567],\n",
       "       [ 0.        , -0.41118197, -0.1023104 ],\n",
       "       [ 0.        ,  0.51722364, -0.16369664],\n",
       "       [ 0.        , -0.66113733, -0.34785536],\n",
       "       [ 0.        , -1.33958759, -2.00528382],\n",
       "       [ 0.        ,  1.76700043,  2.5372979 ],\n",
       "       [ 0.        , -0.83967687, -0.34785536],\n",
       "       [ 0.        , -1.08963223, -0.34785536]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X_train)\n",
    "scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래에 Model Fitting 진행\n",
    "from functools import partial # partial을 이용해 fn과 gradient_fn 구현\n",
    "\n",
    "fn = neg(partial(logistic_log_likelihood, X_train, y_train))\n",
    "gradient_fn = neg_all(partial(logistic_log_gradient, X_train, y_train))\n",
    "\n",
    "beta_0 = [random.random() for _ in range(3)] # 임의의 시작점\n",
    "\n",
    "# 경사 하강법으로 최적화\n",
    "beta_hat = minimize_bgd(fn, gradient_fn, beta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* partial: 함수 속에 들어갈 인자를 미리 지정해 주는 파이썬 내장함수  \n",
    "   \n",
    "   \n",
    "* fn: 목적함수 = log likelihood\n",
    "    - logistic log likelihood 함수에 X_train, y_train 인자를 넣었을 때  \n",
    "      log likelihood 값 찾는 것.\n",
    "    - 위에서 구현한 likelihood 함수의 경우 최댓값을 찾는 함수 (위로 볼록한 형태)   \n",
    "      -> 경사 하강법은 기울기가 가장 가파른 곳을 찾아 \"내려가는\" 알고리즘   \n",
    "      -> 우리는 \"최솟값\"을 찾아야 함  \n",
    "      -> 따라서 negative를 취해줌으로써 fn을 음수 형태로 바꿔주고   \n",
    "         최솟값을 찾는 형태로 바꿔주는 것 \n",
    "\n",
    "\n",
    "* gradient_fn: 최적 그래디언트 값   \n",
    "    - logistic log gradient 함수에 X_train, y_train 인자를 넣었을 때   \n",
    "      최적의 gradient 값을 찾는 것\n",
    "    - fn의 값이 음수가 되었으므로 gradient_fn 값도 음수 형태로 바꾸어 줘야 함 \n",
    "\n",
    "\n",
    "* beta_hat = beta_0 - fn * gradient_fn \n",
    "   - 로지스틱 회귀 모형의 \"회귀 계수\" \n",
    "   - 임의의 시작점에서 목적함수, 최적 그래디언트 값을 고려하여   \n",
    "     최적의 회귀 계수를 찾는 것 \n",
    "   - gradient descent를 통해 찾은 최적 회귀 계수(Beta_hat) 값으로 향후 로지스틱 회귀모형에 피팅하게 될 것 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
